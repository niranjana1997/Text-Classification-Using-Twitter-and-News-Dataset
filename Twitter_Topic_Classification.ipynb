{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niranjana1997/Twitter-Topic-Classification/blob/main/Twitter_Topic_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nePi7e3V-VDl"
      },
      "outputs": [],
      "source": [
        "# Installing tweepy\n",
        "\n",
        "!pip install tweepy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hn9B1jxp_Wi0",
        "outputId": "084c3fff-f010-46d8-98c7-cdeb47dd3c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:File `'code.py'` not found.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'code.py': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Impoerting the code.py which contains the Twitter API tokens\n",
        "%run code.py\n",
        "!rm code.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk-Pup-g_dbD",
        "outputId": "d9ac9f52-dca7-4165-ef5a-ebeb0ae60d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Getting Data using Twitter API"
      ],
      "metadata": {
        "id": "57TK7YNWH25D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TuFBM9ZV_g4U"
      },
      "outputs": [],
      "source": [
        "# # Authenticate to Twitter\n",
        "# auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
        "# auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "\n",
        "# # API object is created\n",
        "# api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# # Tweets are collected using API object\n",
        "\n",
        "# tweets = tweepy.Cursor(api.search, q=\"american football\", lang=\"en\", tweet_mode='extended').items(2500)\n",
        "\n",
        "# # list of data from 'tweets' are created\n",
        "# tweet_data = [[tweet.id, tweet.created_at, tweet.full_text] for tweet in tweets]\n",
        "\n",
        "# # Converting list to DataFrame\n",
        "# tweets_df = pd.DataFrame(tweet_data, columns=[\"id\", \"created_at\", \"text\"])\n",
        "\n",
        "# # converting dataframe to save csv\n",
        "# tweets_df.to_csv(\"american football.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nypSOaRsUf3"
      },
      "source": [
        "#### Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NKtnDL7R-EqC"
      },
      "outputs": [],
      "source": [
        "tweets_df = pd.read_csv(\"american football.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vMuGLI7V6alQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa1d4ba-7dc6-46e4-9abe-26c5a5d65a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# mentions, hashtags, URLs are removed\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: re.sub(r'@\\w+|#\\w+|http\\S+', '', x))\n",
        "\n",
        "# non-alpha numeric values are removed\n",
        "tweets_df['text'] = tweets_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# text is converted to lopwercase\n",
        "tweets_df['text'] = tweets_df['text'].str.lower()\n",
        "\n",
        "# tokenisation\n",
        "tweets_df['tokens'] = tweets_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
        "\n",
        "# stopwords are removed\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "tweets_df['tokens'] = tweets_df['tokens'].apply(lambda x: [word for word in x if word not in stopwords])\n",
        "\n",
        "# ['american', 'football'] are removed manualluy from tokens\n",
        "tweets_df['tokens'] = tweets_df['tokens'].apply(lambda x: [word for word in x if word not in ['american', 'football']])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tweets_df['tokens'] = tweets_df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# tokens are joined\n",
        "tweets_df['processed_text'] = tweets_df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# duplicate columns are removed\n",
        "tweets_df = tweets_df.drop_duplicates(subset='processed_text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Sd_Kyhqp23P1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "5b68e08d-eb2e-4a8c-ed1c-d77086247d13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                       id           created_at  \\\n",
              "0     1631386809975316497  2023-03-02 20:11:39   \n",
              "1     1631386057613664256  2023-03-02 20:08:40   \n",
              "2     1631385869151244293  2023-03-02 20:07:55   \n",
              "3     1631385741879025667  2023-03-02 20:07:25   \n",
              "4     1631385643434516480  2023-03-02 20:07:01   \n",
              "...                   ...                  ...   \n",
              "2486  1630388152861958147  2023-02-28 02:03:21   \n",
              "2489  1630387425842286592  2023-02-28 02:00:28   \n",
              "2494  1630385964420923400  2023-02-28 01:54:39   \n",
              "2497  1630385730945003523  2023-02-28 01:53:44   \n",
              "2499  1630384976444178432  2023-02-28 01:50:44   \n",
              "\n",
              "                                                   text  \\\n",
              "0       furthermore the sounders and atlanta united ...   \n",
              "1     rt  actually fuck off their taking the piss no...   \n",
              "2     rt  the chargers franchise began it all in the...   \n",
              "3     rt  the wyoming black 14 were american footbal...   \n",
              "4      dealer  foxing\\nhow to friend love freefall  ...   \n",
              "...                                                 ...   \n",
              "2486    or when soccer fans call american football h...   \n",
              "2489   skill issue mine was at an american football ...   \n",
              "2494  rt  for the month of february we will be recog...   \n",
              "2497  we dont need much we just need enough  a taxi ...   \n",
              "2499     once again i am not american dont really ca...   \n",
              "\n",
              "                                                 tokens  \\\n",
              "0     [furthermore, sounder, atlanta, united, stripe...   \n",
              "1     [rt, actually, fuck, taking, piss, fan, want, ...   \n",
              "2     [rt, charger, franchise, began, league, 1960, ...   \n",
              "3     [rt, wyoming, black, 14, player, university, w...   \n",
              "4     [dealer, foxing, friend, love, freefall, rainb...   \n",
              "...                                                 ...   \n",
              "2486                       [soccer, fan, call, handegg]   \n",
              "2489  [skill, issue, mine, game, play, back, high, s...   \n",
              "2494  [rt, month, february, recognizing, outstanding...   \n",
              "2497  [dont, need, much, need, enough, taxi, driver,...   \n",
              "2499  [dont, really, care, constitution, lockdown, c...   \n",
              "\n",
              "                                         processed_text  \n",
              "0     furthermore sounder atlanta united stripe logo...  \n",
              "1     rt actually fuck taking piss fan want successf...  \n",
              "2     rt charger franchise began league 1960 name in...  \n",
              "3     rt wyoming black 14 player university wyoming ...  \n",
              "4     dealer foxing friend love freefall rainbow kit...  \n",
              "...                                                 ...  \n",
              "2486                            soccer fan call handegg  \n",
              "2489  skill issue mine game play back high school go...  \n",
              "2494  rt month february recognizing outstanding afri...  \n",
              "2497  dont need much need enough taxi driver said li...  \n",
              "2499  dont really care constitution lockdown country...  \n",
              "\n",
              "[1507 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce43761e-fde9-4e59-ba00-ebe7def23804\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>text</th>\n",
              "      <th>tokens</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1631386809975316497</td>\n",
              "      <td>2023-03-02 20:11:39</td>\n",
              "      <td>furthermore the sounders and atlanta united ...</td>\n",
              "      <td>[furthermore, sounder, atlanta, united, stripe...</td>\n",
              "      <td>furthermore sounder atlanta united stripe logo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1631386057613664256</td>\n",
              "      <td>2023-03-02 20:08:40</td>\n",
              "      <td>rt  actually fuck off their taking the piss no...</td>\n",
              "      <td>[rt, actually, fuck, taking, piss, fan, want, ...</td>\n",
              "      <td>rt actually fuck taking piss fan want successf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1631385869151244293</td>\n",
              "      <td>2023-03-02 20:07:55</td>\n",
              "      <td>rt  the chargers franchise began it all in the...</td>\n",
              "      <td>[rt, charger, franchise, began, league, 1960, ...</td>\n",
              "      <td>rt charger franchise began league 1960 name in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1631385741879025667</td>\n",
              "      <td>2023-03-02 20:07:25</td>\n",
              "      <td>rt  the wyoming black 14 were american footbal...</td>\n",
              "      <td>[rt, wyoming, black, 14, player, university, w...</td>\n",
              "      <td>rt wyoming black 14 player university wyoming ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1631385643434516480</td>\n",
              "      <td>2023-03-02 20:07:01</td>\n",
              "      <td>dealer  foxing\\nhow to friend love freefall  ...</td>\n",
              "      <td>[dealer, foxing, friend, love, freefall, rainb...</td>\n",
              "      <td>dealer foxing friend love freefall rainbow kit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2486</th>\n",
              "      <td>1630388152861958147</td>\n",
              "      <td>2023-02-28 02:03:21</td>\n",
              "      <td>or when soccer fans call american football h...</td>\n",
              "      <td>[soccer, fan, call, handegg]</td>\n",
              "      <td>soccer fan call handegg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2489</th>\n",
              "      <td>1630387425842286592</td>\n",
              "      <td>2023-02-28 02:00:28</td>\n",
              "      <td>skill issue mine was at an american football ...</td>\n",
              "      <td>[skill, issue, mine, game, play, back, high, s...</td>\n",
              "      <td>skill issue mine game play back high school go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2494</th>\n",
              "      <td>1630385964420923400</td>\n",
              "      <td>2023-02-28 01:54:39</td>\n",
              "      <td>rt  for the month of february we will be recog...</td>\n",
              "      <td>[rt, month, february, recognizing, outstanding...</td>\n",
              "      <td>rt month february recognizing outstanding afri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2497</th>\n",
              "      <td>1630385730945003523</td>\n",
              "      <td>2023-02-28 01:53:44</td>\n",
              "      <td>we dont need much we just need enough  a taxi ...</td>\n",
              "      <td>[dont, need, much, need, enough, taxi, driver,...</td>\n",
              "      <td>dont need much need enough taxi driver said li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>1630384976444178432</td>\n",
              "      <td>2023-02-28 01:50:44</td>\n",
              "      <td>once again i am not american dont really ca...</td>\n",
              "      <td>[dont, really, care, constitution, lockdown, c...</td>\n",
              "      <td>dont really care constitution lockdown country...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1507 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce43761e-fde9-4e59-ba00-ebe7def23804')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce43761e-fde9-4e59-ba00-ebe7def23804 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce43761e-fde9-4e59-ba00-ebe7def23804');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "tweets_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LMFgWyDpANUK"
      },
      "outputs": [],
      "source": [
        "tweets_df.to_csv('cleaned_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QE5_BuebSfHK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN52iP7cK8mnMFzuqbdh6vy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}